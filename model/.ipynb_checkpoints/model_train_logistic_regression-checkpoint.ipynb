{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 17:55:49.436944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from helpers import (\n",
    "    preprocessing\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, DateType, FloatType, ArrayType, LongType, MapType, DateType\n",
    "import warnings\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/04 17:55:51 WARN Utils: Your hostname, daholive-Dell-G15-5510 resolves to a loopback address: 127.0.1.1; using 192.168.0.114 instead (on interface wlp0s20f3)\n",
      "22/08/04 17:55:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/daholive/anaconda3/envs/twiiter_tensor/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/daholive/.ivy2/cache\n",
      "The jars for the packages stored in: /home/daholive/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9f34c4ec-1a96-43c4-8048-581da6e65cf0;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/daholive/anaconda3/envs/twiiter_tensor/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound io.delta#delta-core_2.12;1.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;1.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9f34c4ec-1a96-43c4-8048-581da6e65cf0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "22/08/04 17:55:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# instancia spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\",\"8G\") \\\n",
    "    .config(\"spark.driver.memory\",\"8G\") \\\n",
    "    .config(\"spark.executor.cores\",\"12\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base de dados do twitter ja classificada com sentimentos\n",
    "path = \"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/raw_kaggle/TweetsWithTheme_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe twitter com sentimentos classificados\n",
    "dataframe = spark.read.options(delimiter=';',header='True').csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label adjust\n",
    "dataframe = dataframe.withColumn(\"sentiment_map\", \n",
    "    F.when(F.col(\"sentiment\")==\"Negativo\", 0).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe features\n",
    "rdd2 = dataframe.rdd.map(lambda x: (preprocessing(x.tweet_text),len(preprocessing(x.tweet_text).split()),x.sentiment_map))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('features', StringType(), True),\n",
    "    StructField('tokens_count', IntegerType(), True),\n",
    "    StructField('label', IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_features = spark.createDataFrame(rdd2, schema = schema)\n",
    "\n",
    "count_map = F.udf( \n",
    "    lambda x: len(x.split()),\n",
    "    IntegerType()     \n",
    ")\n",
    "\n",
    "df_features = df_features \\\n",
    "    .filter(F.col(\"features\")!=\"-\") \\\n",
    "    .filter( count_map(F.col(\"features\"))<30 ) \\\n",
    "    .dropDuplicates(subset = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_features.sampleBy(\"label\", fractions={0: 1, 1: 0.87}, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# features and labels\n",
    "features = train.select('features').rdd.flatMap(lambda x: x).collect()\n",
    "labels = np.array(train.select('label').rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_features(texto, num_features):\n",
    "    # count vectorizer\n",
    "    X_train = CountVectorizer(max_features=num_features).fit(features).transform(texto)\n",
    "\n",
    "    # tf-idf\n",
    "    return TfidfTransformer().fit(X_train).transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.80      0.74      0.81      0.77      0.78      0.60     19280\n",
      "          1       0.76      0.81      0.74      0.78      0.78      0.61     19018\n",
      "\n",
      "avg / total       0.78      0.78      0.78      0.78      0.78      0.60     38298\n",
      "\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.76      0.73      0.77      0.74      0.75      0.56      8166\n",
      "          1       0.74      0.77      0.73      0.76      0.75      0.56      8248\n",
      "\n",
      "avg / total       0.75      0.75      0.75      0.75      0.75      0.56     16414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## MODEL TESTS  LogisticRegression\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, max_features: int,y: np.array) -> None:\n",
    "    \n",
    "    # tokenizer\n",
    "    # X = tf_idf_features(X,max_features)\n",
    "    X_train = CountVectorizer(max_features=max_features).fit(X).transform(X)\n",
    "    X_ = TfidfTransformer().fit(X_train).transform(X_train)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_, y, train_size=0.7, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    penalty = ['l2']\n",
    "    c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "    max_iter = [10000]\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values,max_iter=max_iter)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "    random_search_cv = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=grid,\n",
    "        n_jobs=-1, \n",
    "        cv=kfold,\n",
    "        n_iter=20\n",
    "    )\n",
    "\n",
    "    random_search_cv.fit(X_train, y_train)\n",
    "    y_pred_train = random_search_cv.predict(X_train)\n",
    "    y_pred_valid = random_search_cv.predict(X_valid)\n",
    "\n",
    "    print(classification_report_imbalanced(y_train, y_pred_train))\n",
    "    print(classification_report_imbalanced(y_valid, y_pred_valid))\n",
    "\n",
    "    return random_search_cv\n",
    "\n",
    "y_train = labels\n",
    "\n",
    "model_lr = train_and_show_scores(features,2000,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto = ['Lula é um péssimo presidente do Brasil','Bolsonaro é ruim']\n",
    "texto = ['Lula é um péssimo presidente do Brasil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5320154, 0.4679846]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.predict_proba(tf_idf_features(texto,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.predict(tf_idf_features(texto,2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction with new tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tweets = \"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/trusted/tweets_preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = spark.read.parquet(path_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------------------\n",
      " author_id     | 1092162170132008961                                               \n",
      " created_at    | 2022-03-15 08:53:32                                               \n",
      " twitter_id    | 1503700925155876879                                               \n",
      " lang          | pt                                                                \n",
      " source        | Twitter for iPhone                                                \n",
      " text          | “a gasolina mais barata é a nossa” homi vai tomar no cu bolsonaro \n",
      " query         | bolsonaro                                                         \n",
      " hashtags_tag  | null                                                              \n",
      " like_count    | 0                                                                 \n",
      " reply_count   | 0                                                                 \n",
      " retweet_count | 0                                                                 \n",
      " created_at_tz | 2022-03-15                                                        \n",
      " text_clean    | a gasolina barata nossa homi vai tomar cu bolsonaro               \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tweets.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author_id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- twitter_id: string (nullable = true)\n",
      " |-- lang: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- query: string (nullable = true)\n",
      " |-- hashtags_tag: string (nullable = true)\n",
      " |-- like_count: integer (nullable = true)\n",
      " |-- reply_count: integer (nullable = true)\n",
      " |-- retweet_count: integer (nullable = true)\n",
      " |-- created_at_tz: date (nullable = true)\n",
      " |-- text_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_pred = df_tweets.rdd.map(lambda x: (x.created_at_tz, x.query, x.text_clean, int(model_lr.predict(tf_idf_features([x.text_clean],2000))[0]) ))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('created_at_tz', DateType(), True),\n",
    "    StructField('candidato', StringType(), True),\n",
    "    StructField('text_clean', StringType(), True),\n",
    "    StructField('sentiment_pred', IntegerType() , True),\n",
    "])\n",
    "\n",
    "df_tweets_pred = spark.createDataFrame(rdd_pred, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets_pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "(df_tweets_pred\n",
    " .write\n",
    " .partitionBy('created_at_tz')\n",
    " .save(\"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/refined/tweets_logistic_regression\", mode='overwrite')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_tweets_pred.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60c0eae9c48b613b38283907a3e5249fb461fae5c3397c9b6eb4a8c20420e31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
