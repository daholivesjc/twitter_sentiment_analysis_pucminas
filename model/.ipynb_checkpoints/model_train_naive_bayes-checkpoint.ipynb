{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    preprocessing\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, DateType, FloatType, ArrayType, LongType, MapType, DateType\n",
    "import warnings\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instancia spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\",\"4G\") \\\n",
    "    .config(\"spark.driver.memory\",\"4G\") \\\n",
    "    .config(\"spark.executor.cores\",\"12\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base de dados do twitter ja classificada com sentimentos\n",
    "path = \"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/raw_kaggle/TweetsWithTheme_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe twitter com sentimentos classificados\n",
    "dataframe = spark.read.options(delimiter=';',header='True').csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label adjust\n",
    "dataframe = dataframe.withColumn(\"sentiment_map\", \n",
    "    F.when(F.col(\"sentiment\")==\"Negativo\", 0).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe features\n",
    "rdd2 = dataframe.rdd.map(lambda x: (preprocessing(x.tweet_text),len(preprocessing(x.tweet_text).split()),x.sentiment_map))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('features', StringType(), True),\n",
    "    StructField('tokens_count', IntegerType(), True),\n",
    "    StructField('label', IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_features = spark.createDataFrame(rdd2, schema = schema)\n",
    "\n",
    "count_map = F.udf( \n",
    "    lambda x: len(x.split()),\n",
    "    IntegerType()     \n",
    ")\n",
    "\n",
    "df_features = df_features \\\n",
    "    .filter(F.col(\"features\")!=\"-\") \\\n",
    "    .filter( count_map(F.col(\"features\"))<30 ) \\\n",
    "    .dropDuplicates(subset = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_features.sampleBy(\"label\", fractions={0: 1, 1: 0.87}, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and labels\n",
    "features = train.select('features').rdd.flatMap(lambda x: x).collect()\n",
    "labels = np.array(train.select('label').rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_features(texto, num_features):\n",
    "    # count vectorizer\n",
    "    X_train = CountVectorizer(max_features=num_features).fit(features).transform(texto)\n",
    "\n",
    "    # tf-idf\n",
    "    return TfidfTransformer().fit(X_train).transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL TESTS  MultinomialNB\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, max_features: int,y: np.array) -> None:\n",
    "    \n",
    "    # tokenizer\n",
    "    X = tf_idf_features(X,max_features)\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.7, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "\n",
    "    alpha = [100, 10, 1.0, 0.1, 0.01]\n",
    "    grid = dict(alpha=alpha)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "    random_search_cv = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=grid,\n",
    "        n_jobs=-1, \n",
    "        cv=kfold,\n",
    "        n_iter=20\n",
    "    )\n",
    "\n",
    "    random_search_cv.fit(X_train, y_train)\n",
    "    y_pred_train = random_search_cv.predict(X_train)\n",
    "    y_pred_valid = random_search_cv.predict(X_valid)\n",
    "\n",
    "    print(classification_report_imbalanced(y_train, y_pred_train))\n",
    "    print(classification_report_imbalanced(y_valid, y_pred_valid))\n",
    "\n",
    "    return random_search_cv\n",
    "\n",
    "y_train = labels\n",
    "\n",
    "model_nb = train_and_show_scores(features,2000,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = ['Lula é um péssimo presidente do Brasil','Bolsonaro é ruim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.predict_proba(tf_idf_features(texto,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.predict(tf_idf_features(texto,2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction with new tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tweets = \"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/trusted/tweets_preprocessing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = spark.read.parquet(path_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_pred = df_tweets.rdd.map(lambda x: (x.created_at_tz, x.query, x.text, x.text_clean, int(model_nb.predict(tf_idf_features([x.text_clean],2000))[0]) ))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('created_at_tz', DateType(), True),\n",
    "    StructField('candidato', StringType(), True),\n",
    "    StructField('text_original', StringType(), True),\n",
    "    StructField('text_clean', StringType(), True),\n",
    "    StructField('sentiment_pred', IntegerType() , True),\n",
    "])\n",
    "\n",
    "df_tweets_pred = spark.createDataFrame(rdd_pred, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "(df_tweets_pred\n",
    " .write\n",
    " .option('mergeSchema', 'true')\n",
    " .option('overwriteSchema', 'true')\n",
    " .save(\"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/refined/tweets_naive_bayes\", mode='overwrite')) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60c0eae9c48b613b38283907a3e5249fb461fae5c3397c9b6eb4a8c20420e31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
