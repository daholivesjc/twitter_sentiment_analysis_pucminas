{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk \n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# model tools\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem.rslp import RSLPStemmer\n",
    "\n",
    "# plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(instancia):\n",
    "\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct)*' ')\n",
    "    \n",
    "    instancia = instancia.lower()\n",
    "    instancia = re.sub('\\d+', '', str(instancia)).replace(\"&gt;\",\" \").replace(\"&lt;\",\" \") \n",
    "    instancia = re.sub(r\"https?:\\/\\/\\S+\",\"\", instancia)\n",
    "    instancia = re.sub(r\"@[A-Za-z0-9\\w]+\",\"\", instancia)\n",
    "    instancia = re.sub(r\"#[A-Za-z0-9\\w]+\",\"\", instancia)\n",
    "    instancia = re.sub('^RT ',' ',instancia)\n",
    "    instancia = re.sub(r\"http\\S+\", \"\", instancia) \n",
    "    instancia = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', instancia)\n",
    "    instancia = instancia.translate(trantab).replace(\"\\n\",\" \")\n",
    "    instancia = unidecode.unidecode(instancia)\n",
    "\n",
    "    # #Lista de  stopwords no idioma portugues\n",
    "    stopwords = [unidecode.unidecode(w) for w in list(set(nltk.corpus.stopwords.words('portuguese')))]\n",
    "\n",
    "    # #guarda no objeto palavras\n",
    "    palavras = [i for i in instancia.split() if not i in stopwords]\n",
    "    \n",
    "    palavras = [re.sub(r'(ha)\\1+', r'\\1',word) for word in palavras]\n",
    "    palavras = [re.sub(r'(uha)\\1+', r'\\1',word) for word in palavras]\n",
    "    palavras = [stemmer.stem(word) for word in palavras]\n",
    "\n",
    "    palavras = \" \".join(palavras) \\\n",
    "        .strip() \\\n",
    "        .replace('\"','') \\\n",
    "        .replace('.','') \\\n",
    "        .replace('-','') \\\n",
    "        .replace('_','') \\\n",
    "        .replace('*','') \\\n",
    "        .replace('>','') \\\n",
    "        .replace('<','') \\\n",
    "        .replace('!','') \\\n",
    "        .replace('?','') \\\n",
    "        .replace('[','') \\\n",
    "        .replace(']','') \\\n",
    "        .replace('\\'','') \\\n",
    "        .replace('rt ','')\n",
    "\n",
    "    return \"-\" if palavras.strip()==\"\" else palavras.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK INSTANCE\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\",\"4G\") \\\n",
    "    .config(\"spark.driver.memory\",\"4G\") \\\n",
    "    .config(\"spark.executor.cores\",\"12\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DADOS\n",
    "dataframe = spark.read.options(\n",
    "    delimiter=';',\n",
    "    header='True').csv(\"/home/daholive/Documents/twitter_ellection_brazil/datasource/TweetsWithTheme_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## PREPROCESSING WITH SPARK\n",
    "###############################################################################\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "dataframe = dataframe.withColumn(\"sentiment_map\", F.when(F.col(\"sentiment\")==\"Negativo\", 0).otherwise(1))\n",
    "\n",
    "rdd2 = dataframe.rdd.map(lambda x: (text_preprocessing(x.tweet_text),x.sentiment_map))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('features', StringType(), True),\n",
    "    StructField('label', StringType(), True),\n",
    "])\n",
    "\n",
    "# create metadata dataframe\n",
    "df_features = spark.createDataFrame(rdd2, schema = schema)\n",
    "\n",
    "count_map = F.udf( \n",
    "    lambda x: len(x.split()),\n",
    "    IntegerType()     \n",
    ")\n",
    "\n",
    "df_features = df_features \\\n",
    "    .filter(F.col(\"features\")!=\"-\") \\\n",
    "    .dropDuplicates(subset = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = df_features.select( \n",
    "    count_map( F.col('features') ).alias('features_count') \n",
    ").toPandas()['features_count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "ax = sns.boxplot(x=feature_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "ax2 = sns.histplot(x=feature_counts, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features \\\n",
    "    .filter( count_map(F.col(\"features\"))<30 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_counts = df_features.select( count_map( F.col('features') ).alias('features_count') ).toPandas()['features_count'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "ax = sns.boxplot(x=new_feature_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "ax2 = sns.histplot(x=new_feature_counts, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset balanceado\n",
    "train = df_features.sampleBy(\"label\", fractions={'0': 1, '1': 0.87}, seed=10)\n",
    "train.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## FEATURE AND LABEL DEFINITION\n",
    "###############################################################################\n",
    "features = train.select('features').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "labels = train.select('label').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## TFIDF\n",
    "###############################################################################\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Bigram Counts\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),  min_df=2, max_df=0.95, max_features=5000)\n",
    "bigram_vectorizer.fit(features)\n",
    "X_train_bigram = bigram_vectorizer.transform(features)\n",
    "\n",
    "# Bigram Tf-Idf\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## MODEL TESTS  LogisticRegression\n",
    "###############################################################################\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.7, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    penalty = ['l2']\n",
    "    c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "    max_iter = [10000]\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values,max_iter=max_iter)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "    random_search_cv = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=grid,\n",
    "        n_jobs=-1, \n",
    "        cv=kfold,\n",
    "        n_iter=20\n",
    "    )\n",
    "\n",
    "    random_search_cv.fit(X_train, y_train)\n",
    "    y_pred_train = random_search_cv.predict(X_train)\n",
    "    y_pred_valid = random_search_cv.predict(X_valid)\n",
    "\n",
    "    print(classification_report_imbalanced(y_train, y_pred_train))\n",
    "    print(classification_report_imbalanced(y_valid, y_pred_valid))\n",
    "\n",
    "    return { \n",
    "        'y_train':y_train,\n",
    "        'y_pred_train':y_pred_train,\n",
    "        'y_valid':y_valid,\n",
    "        'y_pred_valid': y_pred_valid\n",
    "    }\n",
    "\n",
    "y_train = labels\n",
    "\n",
    "data_return = train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFUSION MATRIX - LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretty_confusion_matrix import pp_matrix_from_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_matrix_from_data(data_return['y_train'], data_return['y_pred_train'],columns=['negative','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_matrix_from_data(data_return['y_valid'], data_return['y_pred_valid'],columns=['negative','positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## MODEL TESTS  MultinomialNB\n",
    "###############################################################################\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.7, random_state=42\n",
    "    )\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "\n",
    "    alpha = [100, 10, 1.0, 0.1, 0.01]\n",
    "    grid = dict(alpha=alpha)\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits = 10, shuffle = True)\n",
    "\n",
    "    random_search_cv = RandomizedSearchCV(\n",
    "        estimator=clf,\n",
    "        param_distributions=grid,\n",
    "        n_jobs=-1, \n",
    "        cv=kfold,\n",
    "        n_iter=20\n",
    "    )\n",
    "\n",
    "    random_search_cv.fit(X_train, y_train)\n",
    "    y_pred_train = random_search_cv.predict(X_train)\n",
    "    y_pred_valid = random_search_cv.predict(X_valid)\n",
    "\n",
    "    print(classification_report_imbalanced(y_train, y_pred_train))\n",
    "    print(classification_report_imbalanced(y_valid, y_pred_valid))\n",
    "\n",
    "y_train = labels\n",
    "\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APLICAÇÃO DO TENSORFLOW E REDES NEURAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tensorflow_preprocessing_test_train_split(features, labels, max_words, max_len):\n",
    "    \n",
    "    labels_tf = tf.keras.utils.to_categorical(labels, 2, dtype=\"float32\")\n",
    "    \n",
    "    max_words = max_words\n",
    "    max_len = max_len\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(features)\n",
    "    sequences = tokenizer.texts_to_sequences(features)\n",
    "    tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    #Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        tweets,\n",
    "        labels_tf,\n",
    "        random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def tensorflow_model_fit(max_words, max_len, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # Bidirectional LTSM model\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(max_words, 64, input_length=max_len))\n",
    "    model.add(tf.keras.layers.Bidirectional(layers.LSTM(20,dropout=0.2)))\n",
    "    model.add(tf.keras.layers.Dense(2,activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=\"models/best_model.hdf5\", \n",
    "        monitor='val_acc', \n",
    "        verbose=2,\n",
    "        save_best_only=True, \n",
    "        mode='auto')\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs=10,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[checkpoint])\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "\n",
    "def check_model(model):\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print('Model accuracy: ',test_acc)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    matrix = confusion_matrix(y_test.argmax(axis=1), np.around(predictions, decimals=0).argmax(axis=1))\n",
    "\n",
    "    conf_matrix = pd.DataFrame(matrix, index = [0,1],columns = [0,1])\n",
    "    #Normalizing\n",
    "    conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    plt.figure(figsize = (15,15))\n",
    "    sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 15})\n",
    "\n",
    "\n",
    "def tensorflow_model_test(best_model, tokenizer, token, sentiment_list, max_len):\n",
    "\n",
    "    tokenizer.fit_on_texts(token)\n",
    "\n",
    "    sequence = tokenizer.texts_to_sequences([token])\n",
    "    \n",
    "    test = pad_sequences(sequence, maxlen=max_len)\n",
    "    \n",
    "    print(sentiment_list[np.around(best_model.predict(test), decimals=0).argmax(axis=1)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60c0eae9c48b613b38283907a3e5249fb461fae5c3397c9b6eb4a8c20420e31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
