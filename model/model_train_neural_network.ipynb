{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import (\n",
    "    preprocessing\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, DateType, FloatType, ArrayType, LongType, MapType\n",
    "import warnings\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base de dados do twitter ja classificada com sentimentos\n",
    "path = \"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/raw_kaggle/TweetsWithTheme_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instancia spark\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.executor.memory\",\"4G\") \\\n",
    "    .config(\"spark.driver.memory\",\"4G\") \\\n",
    "    .config(\"spark.executor.cores\",\"12\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe twitter com sentimentos classificados\n",
    "dataframe = spark.read.options(delimiter=';',header='True').csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label adjust\n",
    "dataframe = dataframe.withColumn(\"sentiment_map\", \n",
    "    F.when(F.col(\"sentiment\")==\"Negativo\", 0).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe features\n",
    "rdd2 = dataframe.rdd.map(lambda x: (preprocessing(x.tweet_text),len(preprocessing(x.tweet_text).split()),x.sentiment_map))\n",
    "\n",
    "schema = StructType([       \n",
    "    StructField('features', StringType(), True),\n",
    "    StructField('tokens_count', IntegerType(), True),\n",
    "    StructField('label', IntegerType(), True),\n",
    "])\n",
    "\n",
    "df_features = spark.createDataFrame(rdd2, schema = schema)\n",
    "\n",
    "count_map = F.udf( \n",
    "    lambda x: len(x.split()),\n",
    "    IntegerType()     \n",
    ")\n",
    "\n",
    "df_features = df_features \\\n",
    "    .filter(F.col(\"features\")!=\"-\") \\\n",
    "    .filter( count_map(F.col(\"features\"))<30 ) \\\n",
    "    .dropDuplicates(subset = ['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_features.sampleBy(\"label\", fractions={0: 1, 1: 0.87}, seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and labels\n",
    "features = train.select('features').rdd.flatMap(lambda x: x).collect()\n",
    "labels = np.array(train.select('label').rdd.flatMap(lambda x: x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow - tokenizacao\n",
    "import tensorflow_datasets as tfds\n",
    "import random\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(features, target_vocab_size=2**16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow - padding\n",
    "data_inputs = [tokenizer.encode(sentence) for sentence in features]\n",
    "\n",
    "max_len = max([len(sentence) for sentence in data_inputs])\n",
    "\n",
    "data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n",
    "                                                            value = 0,\n",
    "                                                            padding = 'post',\n",
    "                                                            maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(data_inputs,\n",
    "                                                                        labels,\n",
    "                                                                        test_size=0.3,\n",
    "                                                                        stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model build\n",
    "class DCNN(tf.keras.Model): \n",
    "\n",
    "  def __init__(self,\n",
    "               vocab_size,\n",
    "               emb_dim=128,\n",
    "               nb_filters=50,\n",
    "               ffn_units=512, \n",
    "               nb_classes=2,\n",
    "               dropout_rate=0.1,\n",
    "               training=True,\n",
    "               name=\"dcnn\"):\n",
    "      \n",
    "    super(DCNN, self).__init__(name=name)\n",
    "    \n",
    "    self.embedding = layers.Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding='same', activation='relu')\n",
    "    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding='same', activation='relu')\n",
    "    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding='same', activation='relu')\n",
    "\n",
    "    self.pool = layers.GlobalMaxPool1D()\n",
    "\n",
    "    self.dense_1 = layers.Dense(units = ffn_units, activation = 'relu')\n",
    "\n",
    "    self.dropout = layers.Dropout(rate = dropout_rate)\n",
    "    \n",
    "    if nb_classes == 2:\n",
    "      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n",
    "    else:\n",
    "      self.last_dense = layers.Dense(units = nb_classes, activation = 'softmax')\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    x = self.embedding(inputs) \n",
    "    x_1 = self.bigram(x) \n",
    "    x_1 = self.pool(x_1) \n",
    "    x_2 = self.trigram(x) \n",
    "    x_2 = self.pool(x_2) \n",
    "    x_3 = self.fourgram(x) \n",
    "    x_3 = self.pool(x_3) \n",
    "\n",
    "    merged = tf.concat([x_1, x_2, x_3], axis = -1) # (batch_size, 3 * nb_filters)\n",
    "\n",
    "    merged = self.dense_1(merged)\n",
    "    merged = self.dropout(merged, training)\n",
    "    output = self.last_dense(merged)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables init\n",
    "vocab_size = tokenizer.vocab_size\n",
    "emb_dim = 200 \n",
    "nb_filters = 100 \n",
    "ffn_units = 256 # 256\n",
    "batch_size = 64 \n",
    "nb_classes = len(set(train_labels)) \n",
    "dropout_rate = 0.2\n",
    "nb_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tensorflow parameters for local GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model instance\n",
    "Dcnn = DCNN(\n",
    "    vocab_size=vocab_size, \n",
    "    emb_dim=emb_dim, \n",
    "    nb_filters=nb_filters,\n",
    "    ffn_units=ffn_units, \n",
    "    nb_classes=nb_classes, \n",
    "    dropout_rate=dropout_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compile\n",
    "Dcnn.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint\n",
    "checkpoint_path = \"/home/daholive/Documents/twitter_ellection_brazil_v2/model/checkpoints\"\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn) # passando o objeto que iremos salvar\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "if ckpt_manager.latest_checkpoint: # verifica se existe um ultimo checkpoint salvo\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint) # restaura o ultimo checkpoint salvo\n",
    "    print('Latest checkpoint restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model fit\n",
    "history = Dcnn.fit(\n",
    "    train_inputs, \n",
    "    train_labels,\n",
    "    batch_size = batch_size,\n",
    "    epochs = nb_epochs,\n",
    "    verbose = 1,\n",
    "    validation_split = 0.10\n",
    ")\n",
    "ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model available\n",
    "results = Dcnn.evaluate(\n",
    "    test_inputs, \n",
    "    test_labels, \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "y_pred_test = Dcnn.predict(test_inputs)\n",
    "y_pred_test = (y_pred_test > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(test_labels, y_pred_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution history\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart - training and validation data versus loss progress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss progress during training and validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Losses')\n",
    "plt.legend(['Training loss', 'Validation loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart - training and validation data versus accuracy progress\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy progress during training and validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Training accuracy', 'Validation accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict - 1\n",
    "text = 'Lula é o melhor presidente'\n",
    "text = tokenizer.encode(text) # texto tokenizado\n",
    "\n",
    "Dcnn(np.array([text]), training=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict - 2\n",
    "text = 'Bolsonaro não é de nada'\n",
    "text = tokenizer.encode(text) # texto tokenizado\n",
    "\n",
    "Dcnn(np.array([text]), training=False).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWIITER - DADOS RECENTES - APLICAÇÃO REDES NEURAIS CONVOLUCIONAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.abspath(os.path.join('..', ''))\n",
    "df_twitter = spark.read.parquet(path+\"/datasource/trusted/tweets_preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ = df_twitter.rdd.map(lambda x: (\n",
    "    {\n",
    "        'twitter_id': x.twitter_id,\n",
    "        'candidato': x.query,\n",
    "        'text': x.text,\n",
    "        'created_at_tz':x.created_at_tz, \n",
    "        'text_clean': x.text_clean\n",
    "    }\n",
    ")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = [ {\n",
    "    'twitter_id':arr['twitter_id'],\n",
    "    'created_at_tz':arr['created_at_tz'],\n",
    "    'candidato':arr['candidato'],\n",
    "    'text_original':arr['text'], \n",
    "    'text_clean':arr['text_clean'], \n",
    "    'sentiment_tax': float(Dcnn(np.array([tokenizer.encode(arr['text_clean'])]), training=False).numpy()[0][0]),\n",
    "    'sentiment': 1 if float(Dcnn(np.array([tokenizer.encode(arr['text_clean'])]), training=False).numpy()[0][0])>0.5 else 0\n",
    "  } for arr in features_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "(df\n",
    " .write\n",
    " .option('mergeSchema', 'true')\n",
    " .option('overwriteSchema', 'true')\n",
    " .save(\"/home/daholive/Documents/twitter_ellection_brazil_v2/datasource/refined/tweets_redes_neurais_convolucionais\", mode='overwrite')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60c0eae9c48b613b38283907a3e5249fb461fae5c3397c9b6eb4a8c20420e31a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
